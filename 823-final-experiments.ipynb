{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport imblearn\ndf = pd.read_csv(\"../input/mimic3extracted/mimic-iii-823.csv\")\n#dropped DOB, and ADMIT_TIME since info in other variables\n#dropped diagnosis since to many variables when onehot encoded, partial info encoded in other variables\ndf = df.drop(['DOB', 'ADMIT_TIME', \"DIAGNOSIS\", 'HADM_ID', 'SUBJECT_ID'], axis=1)\ndf = pd.get_dummies(df, columns=['GENDER'], prefix=\"gender_\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['ADMISSION_TYPE'], prefix=\"admit_type\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['ADMISSION_LOCATION'], prefix=\"admit_location\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['INSURANCE'], prefix=\"insurance\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['LANGUAGE'], prefix=\"LANGUAGE_\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['RELIGION'], prefix=\"RELIGION_\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['MARITAL_STATUS'], prefix=\"MARITAL_STATUS_\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['ETHNICITY'], prefix=\"ETHNICITY_\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['month_since_last_visit'], prefix=\"month_since_last_visit\")\nprint(df.shape)\n\n\ndf['year_since_last_visit'] = df['year_since_last_visit'].fillna(df['year_since_last_visit'].max())\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nfor element in df.select_dtypes(include=['int64']).columns:\n    df[[element]] = scaler.fit_transform(df[[element]])\n\nfor element in df.select_dtypes(include=['float64']).columns:\n    df[[element]] = scaler.fit_transform(df[[element]])\n    \n\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-26T00:45:04.657710Z","iopub.execute_input":"2022-04-26T00:45:04.658040Z","iopub.status.idle":"2022-04-26T00:45:09.540471Z","shell.execute_reply.started":"2022-04-26T00:45:04.657958Z","shell.execute_reply":"2022-04-26T00:45:09.539525Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#install required packages\n!pip install interpret\n!pip install pygam\n!pip install flit","metadata":{"execution":{"iopub.status.busy":"2022-04-26T00:45:09.542121Z","iopub.execute_input":"2022-04-26T00:45:09.542368Z","iopub.status.idle":"2022-04-26T00:45:51.096205Z","shell.execute_reply.started":"2022-04-26T00:45:09.542337Z","shell.execute_reply":"2022-04-26T00:45:51.095369Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"###### dataset no changes\n\nfrom matplotlib import pyplot\nfrom numpy import mean\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom interpret import set_visualize_provider\nfrom interpret.provider import InlineProvider\nset_visualize_provider(InlineProvider())\n\nfrom interpret.glassbox import ExplainableBoostingClassifier\nfrom interpret import show\nfrom pygam import LinearGAM, s, f\n\ntop_AUC = 0\nbest_model = None\n# define dataset\nX = df.drop([\"HOSPITAL_EXPIRE_FLAG\"], axis=1)\ny = df[\"HOSPITAL_EXPIRE_FLAG\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 1)\n# define pipeline\nk = 1\nprint(\"random forest results\")\nwhile k < 100:\n    # evaluate pipeline\n    model = RandomForestClassifier(max_depth=k, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"SVM results\")\nk = 1\nwhile k < 8:\n    # evaluate pipeline\n    model = SVC(degree=k, random_state = 0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"GradientBoostingClassifier results\")\nk = 1\nwhile k < 7:\n    # evaluate pipeline\n    model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=k, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"AdaBoostClassifier results\")\nk = 15\nwhile k < 20:\n    # evaluate pipeline\n    model = AdaBoostClassifier(n_estimators=k*25, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"Knn results\")\nk = 3\nwhile k < 10:\n    # evaluate pipeline\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"MLP results\")\nk = 1\nwhile k < 5:\n    # evaluate pipeline\n    model = MLPClassifier(solver='adam', hidden_layer_sizes=(50*k,50*k, 50*k), random_state=1)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nebm = ExplainableBoostingClassifier(random_state=1)\nebm.fit(X_train, y_train)\ny_pred = ebm.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(roc_auc_score(y_test, y_pred))\n\ntry:\n    gam = LinearGAM(s(0) + s(1) + f(2)).fit(X_train, y_train)\n    y_pred = gam.predict(X_test)\n    y_temp = y_pred > 0.5\n    print(accuracy_score(y_test, y_temp))\n    print(roc_auc_score(y_test, y_temp))\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\nexcept:\n    gam = LinearGAM(s(0) + s(1) +s(2)+ f(3)).fit(X_train, y_train)\n    y_pred = gam.predict(X_test)\n    y_temp = y_pred > 0.5\n    print(accuracy_score(y_test, y_temp))\n    print(roc_auc_score(y_test, y_temp))\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\n# dataset randomundersampling\n\nfrom matplotlib import pyplot\nfrom numpy import mean\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom interpret import set_visualize_provider\nfrom interpret.provider import InlineProvider\nset_visualize_provider(InlineProvider())\n\nfrom interpret.glassbox import ExplainableBoostingClassifier\nfrom interpret import show\nfrom pygam import LinearGAM, s, f\n\ntop_AUC = 0\nbest_model = None\n# define dataset\nX = df.drop([\"HOSPITAL_EXPIRE_FLAG\"], axis=1)\ny = df[\"HOSPITAL_EXPIRE_FLAG\"]\nrus = RandomUnderSampler(random_state=42)\nX_res, y_res = rus.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state = 0)\n# define pipeline\nk = 1\nprint(\"random forest results\")\nwhile k < 100:\n    # evaluate pipeline\n    model = RandomForestClassifier(max_depth=k, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"SVM results\")\nk = 1\nwhile k < 8:\n    # evaluate pipeline\n    model = SVC(degree=k, random_state = 0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"GradientBoostingClassifier results\")\nk = 1\nwhile k < 7:\n    # evaluate pipeline\n    model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=k, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"AdaBoostClassifier results\")\nk = 15\nwhile k < 20:\n    # evaluate pipeline\n    model = AdaBoostClassifier(n_estimators=k*25, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"Knn results\")\nk = 3\nwhile k < 10:\n    # evaluate pipeline\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"MLP results\")\nk = 1\nwhile k < 5:\n    # evaluate pipeline\n    model = MLPClassifier(solver='adam', hidden_layer_sizes=(50*k,50*k, 50*k), random_state=1)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nebm = ExplainableBoostingClassifier(random_state=1)\nebm.fit(X_train, y_train)\ny_pred = ebm.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(roc_auc_score(y_test, y_pred))\n\ntry:\n    gam = LinearGAM(s(0) + s(1) + f(2)).fit(X_train, y_train)\n    y_pred = gam.predict(X_test)\n    y_temp = y_pred > 0.5\n    print(accuracy_score(y_test, y_temp))\n    print(roc_auc_score(y_test, y_temp))\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\nexcept:\n    gam = LinearGAM(s(0) + s(1) +s(2)+ f(3)).fit(X_train, y_train)\n    y_pred = gam.predict(X_test)\n    y_temp = y_pred > 0.5\n    print(accuracy_score(y_test, y_temp))\n    print(roc_auc_score(y_test, y_temp))\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model","metadata":{"execution":{"iopub.status.busy":"2022-04-26T00:45:51.098199Z","iopub.execute_input":"2022-04-26T00:45:51.098544Z","iopub.status.idle":"2022-04-26T01:12:10.471995Z","shell.execute_reply.started":"2022-04-26T00:45:51.098498Z","shell.execute_reply":"2022-04-26T01:12:10.470885Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# dataset SMOTE \nimport numpy as np # linear algebra\nimport pandas as pd\nimport imblearn\ndf = pd.read_csv(\"../input/mimic3extracted/mimic-iii-823.csv\")\n#dropped DOB, and ADMIT_TIME since info in other variables\n#dropped diagnosis since to many variables when onehot encoded, partial info encoded in other variables\ndf = df.drop(['DOB', 'ADMIT_TIME', \"DIAGNOSIS\", 'HADM_ID', 'SUBJECT_ID'], axis=1)\ndf = pd.get_dummies(df, columns=['GENDER'], prefix=\"gender_\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['ADMISSION_TYPE'], prefix=\"admit_type\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['ADMISSION_LOCATION'], prefix=\"admit_location\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['INSURANCE'], prefix=\"insurance\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['LANGUAGE'], prefix=\"LANGUAGE_\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['RELIGION'], prefix=\"RELIGION_\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['MARITAL_STATUS'], prefix=\"MARITAL_STATUS_\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['ETHNICITY'], prefix=\"ETHNICITY_\")\nprint(df.shape)\ndf = pd.get_dummies(df, columns=['month_since_last_visit'], prefix=\"month_since_last_visit\")\nprint(df.shape)\n\n\ndf['year_since_last_visit'] = df['year_since_last_visit'].fillna(df['year_since_last_visit'].max())\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nfor element in df.select_dtypes(include=['int64']).columns:\n    df[[element]] = scaler.fit_transform(df[[element]])\n\nfor element in df.select_dtypes(include=['float64']).columns:\n    df[[element]] = scaler.fit_transform(df[[element]])\n    \n\n    \n\nfrom matplotlib import pyplot\nfrom numpy import mean\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom interpret import set_visualize_provider\nfrom interpret.provider import InlineProvider\nset_visualize_provider(InlineProvider())\n\nfrom interpret.glassbox import ExplainableBoostingClassifier\nfrom interpret import show\nfrom pygam import LinearGAM, s, f\n\ntop_AUC = 0\nbest_model = None\n# define dataset\nX = df.drop([\"HOSPITAL_EXPIRE_FLAG\"], axis=1)\ny = df[\"HOSPITAL_EXPIRE_FLAG\"]\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state = 1)\n# define pipeline\nk = 1\nprint(\"random forest results\")\nwhile k < 100:\n    # evaluate pipeline\n    model = RandomForestClassifier(max_depth=k, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"SVM results\")\nk = 1\nwhile k < 8:\n    # evaluate pipeline\n    model = SVC(degree=k, random_state = 0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"GradientBoostingClassifier results\")\nk = 1\nwhile k < 7:\n    # evaluate pipeline\n    model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=k, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"AdaBoostClassifier results\")\nk = 15\nwhile k < 20:\n    # evaluate pipeline\n    model = AdaBoostClassifier(n_estimators=k*25, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"Knn results\")\nk = 3\nwhile k < 10:\n    # evaluate pipeline\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nprint(\"MLP results\")\nk = 1\nwhile k < 5:\n    # evaluate pipeline\n    model = MLPClassifier(solver='adam', hidden_layer_sizes=(50*k,50*k, 50*k), random_state=1)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(k)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    k = k + 1\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\n\nebm = ExplainableBoostingClassifier(random_state=0)\nebm.fit(X_train, y_train)\ny_pred = ebm.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(roc_auc_score(y_test, y_pred))\nif roc_auc_score(y_test, y_pred) > top_AUC:\n    top_AUC = roc_auc_score(y_test, y_pred)\n    best_model = model\n\ntry:\n    gam = LinearGAM(s(0) + s(1) + f(2)).fit(X_train, y_train)\n    y_pred = gam.predict(X_test)\n    y_temp = y_pred > 0.5\n    print(accuracy_score(y_test, y_temp))\n    print(roc_auc_score(y_test, y_temp))\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model\nexcept:\n    gam = LinearGAM(s(0) + s(1) +s(2)+ f(3)).fit(X_train, y_train)\n    y_pred = gam.predict(X_test)\n    y_temp = y_pred > 0.5\n    print(accuracy_score(y_test, y_temp))\n    print(roc_auc_score(y_test, y_temp))\n    if roc_auc_score(y_test, y_pred) > top_AUC:\n        top_AUC = roc_auc_score(y_test, y_pred)\n        best_model = model","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:12:10.479761Z","iopub.execute_input":"2022-04-26T01:12:10.480411Z","iopub.status.idle":"2022-04-26T01:45:33.948975Z","shell.execute_reply.started":"2022-04-26T01:12:10.480356Z","shell.execute_reply":"2022-04-26T01:45:33.948033Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ensemble of top 3 models\nfrom sklearn.ensemble import VotingClassifier\n\n#clf1 = LinearGAM(s(0) + s(1) + s(2) + f(3)).fit(X_train, y_train)\n#clf1_prob = LinearGAM(s(0) + s(1) + f(2)).fit(X_train, y_train)\nclf2 = RandomForestClassifier(max_depth=50, random_state=1)\nclf3 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=6, random_state=0)\n#clf4 = MLPClassifier(solver='adam', hidden_layer_sizes=(150,150, 150), random_state=1)\nclf5 = ExplainableBoostingClassifier(random_state=1)\n\neclf_hard = VotingClassifier(estimators=[('rf', clf2), ('gnb', clf3), (\"ebc\", clf5)],voting='hard')\neclf_soft = VotingClassifier(estimators=[('rf', clf2), ('gnb', clf3), (\"ebc\", clf5)],voting='soft')\n\n\neclf_hard = eclf_hard.fit(X_train, y_train)\neclf_soft = eclf_soft.fit(X_train, y_train)\n\nprint(\"hard voting result\")\ny_pred = eclf_hard.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(roc_auc_score(y_test, y_pred))\n\nprint(\"soft voting result\")\ny_pred = eclf_soft.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(roc_auc_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:45:33.950436Z","iopub.execute_input":"2022-04-26T01:45:33.950705Z","iopub.status.idle":"2022-04-26T01:50:12.136273Z","shell.execute_reply.started":"2022-04-26T01:45:33.950665Z","shell.execute_reply":"2022-04-26T01:50:12.135337Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#ensemble test for 3 random forests\nfrom sklearn.ensemble import VotingClassifier\n\nclf1 = RandomForestClassifier(max_depth=50, random_state=1)\nclf2 = RandomForestClassifier(max_depth=50, random_state=2)\nclf3 = RandomForestClassifier(max_depth=50, random_state=3)\n\neclf_hard = VotingClassifier(estimators=[('rf1', clf1), ('rf2', clf2), ('rf3', clf3)],voting='hard')\neclf_soft = VotingClassifier(estimators=[('rf1', clf1), ('rf2', clf2), ('rf3', clf3)],voting='soft')\n\nclf1 = clf1.fit(X_train, y_train)\nclf2 = clf2.fit(X_train, y_train)\nclf3 = clf3.fit(X_train, y_train)\neclf_hard = eclf_hard.fit(X_train, y_train)\neclf_soft = eclf_soft.fit(X_train, y_train)\n\nprint(\"SVM result:\")\ny_pred = clf1.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(roc_auc_score(y_test, y_pred))\n\nprint(\"RF result:\")\ny_pred = clf2.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(roc_auc_score(y_test, y_pred))\n\nprint(\"MLP results\")\ny_pred = clf3.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(roc_auc_score(y_test, y_pred))\n\nprint(\"hard voting result\")\ny_pred = eclf_hard.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(roc_auc_score(y_test, y_pred))\n\nprint(\"soft voting result\")\ny_pred = eclf_soft.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(roc_auc_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:50:12.138003Z","iopub.execute_input":"2022-04-26T01:50:12.138332Z","iopub.status.idle":"2022-04-26T01:50:39.375164Z","shell.execute_reply.started":"2022-04-26T01:50:12.138289Z","shell.execute_reply":"2022-04-26T01:50:39.374044Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#extract features from top performing model\nfeature_importances = pd.DataFrame(best_model.feature_importances_,index = list(X.columns), columns=['importance']).sort_values('importance', ascending=False)\nprint(feature_importances)\n\ntop_5 = feature_importances.head(5).index\ntop_10 = feature_importances.head(10).index\ntop_15 = feature_importances.head(15).index\ntop_20 = feature_importances.head(20).index\ntop_25 = feature_importances.head(25).index\ntop_30 = feature_importances.head(30).index\ntop_35 = feature_importances.head(35).index\ntop_40 = feature_importances.head(40).index\ntop_45 = feature_importances.head(45).index\ntop_50 = feature_importances.head(50).index\ntop_features = [top_5, top_10, top_15, top_20, top_25,top_30, top_35,top_40, top_45, top_50]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:50:39.376852Z","iopub.execute_input":"2022-04-26T01:50:39.377272Z","iopub.status.idle":"2022-04-26T01:50:39.415909Z","shell.execute_reply.started":"2022-04-26T01:50:39.377224Z","shell.execute_reply":"2022-04-26T01:50:39.414961Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"top_AUC_rf = 0\nbest_model_rf = None\n\ntop_AUC_svm = 0\nbest_model_svm = None\n\ntop_AUC_gbc = 0\nbest_model_gbc = None\n\ntop_AUC_knn = 0\nbest_model_knn = None\n\ntop_AUC_ada = 0\nbest_model_ada = None\n\ntop_AUC_mlp = 0\nbest_model_mlp = None\n\ntop_AUC_ebc = 0\nbest_model_ebc = None\n\ntop_AUC_gam = 0\nbest_model_gam = None\n\nfor element in top_features:\n    # define dataset\n    X = df[element]\n    y = df[\"HOSPITAL_EXPIRE_FLAG\"]\n    sm = SMOTE(random_state=42)\n    X_res, y_res = sm.fit_resample(X, y)\n    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state = 1)\n    # define pipeline\n    k = 1\n    while k < len(element) and k < 35:\n    # define pipeline\n        print(\"random forest results for:\" + str(len(element)))\n        # evaluate pipeline\n        model = RandomForestClassifier(max_depth=k, random_state=0)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        print(k)\n        print(accuracy_score(y_test, y_pred))\n        print(roc_auc_score(y_test, y_pred))\n        k = k + 1\n        if roc_auc_score(y_test, y_pred) > top_AUC_rf:\n            top_AUC_rf = roc_auc_score(y_test, y_pred)\n            best_model_rf = model\n            \n    print(\"SVM results\")\n    k = 1\n    while k < 8:\n        # evaluate pipeline\n        model = SVC(degree=k)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        print(k)\n        print(accuracy_score(y_test, y_pred))\n        print(roc_auc_score(y_test, y_pred))\n        k = k + 1\n        if roc_auc_score(y_test, y_pred) > top_AUC_svm:\n            top_AUC_svm = roc_auc_score(y_test, y_pred)\n            best_model_svm = model\n\n    print(\"GradientBoostingClassifier results\")\n    k = 1\n    while k < 7:\n        # evaluate pipeline\n        model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=k, random_state=0)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        print(k)\n        print(accuracy_score(y_test, y_pred))\n        print(roc_auc_score(y_test, y_pred))\n        k = k + 1\n        if roc_auc_score(y_test, y_pred) > top_AUC_gbc:\n            top_AUC_gbc = roc_auc_score(y_test, y_pred)\n            best_model_gbc = model\n\n    print(\"AdaBoostClassifier results\")\n    k = 15\n    while k < 20:\n        # evaluate pipeline\n        model = AdaBoostClassifier(n_estimators=k*25, random_state=0)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        print(k)\n        print(accuracy_score(y_test, y_pred))\n        print(roc_auc_score(y_test, y_pred))\n        k = k + 1\n        if roc_auc_score(y_test, y_pred) > top_AUC_ada:\n            top_AUC_ada = roc_auc_score(y_test, y_pred)\n            best_model_ada = model\n\n    print(\"Knn results\")\n    k = 3\n    while k < 10:\n        # evaluate pipeline\n        model = KNeighborsClassifier(n_neighbors=k)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        print(k)\n        print(accuracy_score(y_test, y_pred))\n        print(roc_auc_score(y_test, y_pred))\n        k = k + 1\n        if roc_auc_score(y_test, y_pred) > top_AUC_knn:\n            top_AUC_knn = roc_auc_score(y_test, y_pred)\n            best_model_knn = model\n\n    print(\"MLP results\")\n    k = 1\n    while k < 8:\n        # evaluate pipeline\n        model = MLPClassifier(solver='adam', hidden_layer_sizes=(25*k,25*k, 25*k), random_state=1)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        print(k)\n        print(accuracy_score(y_test, y_pred))\n        print(roc_auc_score(y_test, y_pred))\n        k = k + 1\n        if roc_auc_score(y_test, y_pred) > top_AUC_mlp:\n            top_AUC_mlp = roc_auc_score(y_test, y_pred)\n            best_model_mlp = model\n\n    model = ExplainableBoostingClassifier(random_state=1)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(accuracy_score(y_test, y_pred))\n    print(roc_auc_score(y_test, y_pred))\n    \n    if roc_auc_score(y_test, y_pred) > top_AUC_ebc:\n        top_AUC_ebc = roc_auc_score(y_test, y_pred)\n        best_model_ebc = model\n    \n    try:\n        model = LinearGAM(s(0) + s(1) + f(2)).fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        y_temp = y_pred > 0.5\n        print(accuracy_score(y_test, y_temp))\n        print(roc_auc_score(y_test, y_temp))\n    except:\n        model = LinearGAM(s(0) + s(1) + s(2) + f(3)).fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        y_temp = y_pred > 0.5\n        print(accuracy_score(y_test, y_temp))\n        print(roc_auc_score(y_test, y_temp))\n        \n    if roc_auc_score(y_test, y_pred) > top_AUC_gam:\n        top_AUC_gam = roc_auc_score(y_test, y_pred)\n        best_model_gam = model\n    print(\"done:\" + str(element))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:50:39.418493Z","iopub.execute_input":"2022-04-26T01:50:39.418845Z","iopub.status.idle":"2022-04-26T03:05:04.824246Z","shell.execute_reply.started":"2022-04-26T01:50:39.418792Z","shell.execute_reply":"2022-04-26T03:05:04.823178Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"ebm_global = best_model_ebc.explain_global()\nshow(ebm_global)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T03:05:04.826010Z","iopub.execute_input":"2022-04-26T03:05:04.826255Z","iopub.status.idle":"2022-04-26T03:05:07.038257Z","shell.execute_reply.started":"2022-04-26T03:05:04.826223Z","shell.execute_reply":"2022-04-26T03:05:07.036747Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(ebm_global)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T03:37:33.728582Z","iopub.execute_input":"2022-04-26T03:37:33.728929Z","iopub.status.idle":"2022-04-26T03:37:33.734485Z","shell.execute_reply.started":"2022-04-26T03:37:33.728897Z","shell.execute_reply":"2022-04-26T03:37:33.733628Z"},"trusted":true},"execution_count":10,"outputs":[]}]}